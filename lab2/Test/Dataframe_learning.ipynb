{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import broadcast\n",
    "import sys\n",
    "import pandas\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Learning\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# From this video\n",
    "# https://youtu.be/_1byVWTEK1s\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "sqlContext = SQLContext(spark_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.read.json(\"/home/ubuntu/LDSA/lab2/Test/zips.json\").createOrReplaceTempView(\"zip\")\n",
    "#sqlContext.read.json(\"/home/ubuntu/LDSA/lab2/Test/zips.json\").registerTempTable(\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"DESCRIBE zip\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.table(\"zip\").withColumnRenamed(\"_id\", \"zip\").createOrReplaceTempView(\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT COUNT(zip), SUM(pop), city FROM zip WHERE state = 'IL' GROUP BY city ORDER BY SUM(pop) DESC LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"CACHE TABLE zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are from: http://changhsinlee.com/pyspark-udf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "square_udf_int = udf(lambda z: square(z), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float type output\n",
    "from pyspark.sql.types import FloatType\n",
    "square_udf_float = udf(lambda z: square(z), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "def square_list(x):\n",
    "    return [float(val)**2 for val in x]\n",
    "\n",
    "square_list_udf = udf(lambda y: square_list(y), ArrayType(FloatType()))\n",
    "\n",
    "#df.select('integer_arrays', square_list_udf('integer_arrays')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def convert_ascii(number):\n",
    "    return [number, string.ascii_letters[number]]\n",
    "\n",
    "convert_ascii(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_schema = StructType([\n",
    "    StructField('number', IntegerType(), nullable=False),\n",
    "    StructField('letters', StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "spark_convert_ascii = udf(lambda z: convert_ascii(z), array_schema)\n",
    "\n",
    "#df_ascii = df.select('integers', spark_convert_ascii('integers').alias('ascii_map'))\n",
    "#df_ascii.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "d_np = pd.DataFrame({'int_arrays': [[1,2,3], [4,5]]})\n",
    "df_np = spark.createDataFrame(d_np)\n",
    "df_np.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://docs.databricks.com/spark/latest/spark-sql/udf-in-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared(s):\n",
    "  return s * s\n",
    "sqlContext.udf.register(\"squaredWithPython\", squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "def squared_typed(s):\n",
    "  return s * s\n",
    "sqlContext.udf.register(\"squaredWithPython\", squared, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.range(1, 20).registerTempTable(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select id, squaredWithPython(id) as id_squared from test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * FROM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "squared_udf = udf(squared, LongType())\n",
    "df = sqlContext.table(\"test\")\n",
    "display(df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of key range\n",
    "from range_key_dict import RangeKeyDict\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    range_key_dict = RangeKeyDict({\n",
    "        (0, 100): 'A',\n",
    "        (100, 200): 'B',\n",
    "        (200, 300): 'C',\n",
    "    })\n",
    "\n",
    "    # test normal case\n",
    "    assert range_key_dict[70] == 'A'\n",
    "    assert range_key_dict[170] == 'B'\n",
    "    assert range_key_dict[270] == 'C'\n",
    "\n",
    "    # test case when the number is float\n",
    "    assert range_key_dict[70.5] == 'A'\n",
    "\n",
    "    # test case not in the range, with default value\n",
    "    assert range_key_dict.get(1000, 'D') == 'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://github.com/benblamey/jupyters/blob/master/ben-spark-master/jupyter/Teaching/Lecture2_Spark_DataFrames_Weather_Demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_WND_to_WND_SPEED_MS = udf(WND_to_WND_SPEED_MS, StringType())\n",
    "\n",
    "\n",
    "data_frame_with_wnd_speed = data_frame.withColumn(\"WND_SPEED_MS\",udf_WND_to_WND_SPEED_MS(\"WND\"))\n",
    "\n",
    "# .filter()\n",
    "\n",
    "# 9999: missing (with scale factor of 10)\n",
    "data_frame_with_wnd_speed = data_frame_with_wnd_speed.filter(data_frame_with_wnd_speed['WND_SPEED_MS'] < 999)\n",
    "\n",
    "#wnd_split = pyspark.sql.functions.split(data_frame['WND'], ',')\n",
    "#data_frame_with_wnd_speed = data_frame.withColumn('WND_SPEED_MS', wnd_split.getItem(3))\n",
    "\n",
    "data_frame_with_wnd_speed.select('WND', 'WND_SPEED_MS').show()\n",
    "\n",
    "data_frame_with_wnd_speed.select('WND_SPEED_MS').summary().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
